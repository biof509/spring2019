{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7\n",
    "## Linear regression.\n",
    "  \n",
    "  \n",
    "Ayal Gussow, 03/15/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's have a look at *Ames* housing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ames = pd.read_table(\"preprocessed_ames.csv\", sep=\",\")\n",
    "ames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Focus on features, outcome\n",
    "#X = ames[[\"LotFrontage\", \"total_sf\"]]\n",
    "#y = ames.SalePrice\n",
    "\n",
    "#X.head()\n",
    "#y.head()\n",
    "\n",
    "# Let's divide this data into training and test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    X, y, test_size=0.20, random_state=213\n",
    "#)\n",
    "\n",
    "#X_train\n",
    "#X_test\n",
    "#y_train\n",
    "#y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sklearn makes life easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#print(X_train.head())\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train) # train\n",
    "#print(lr.intercept_, lr.coef_)\n",
    "#preds = lr.predict(X_test) #predict\n",
    "\n",
    "# Assessments\n",
    "#print(preds)\n",
    "#print(\"\\n\".join([\"{}\\t{}\\t{}\".format(pred, truth, round(abs(pred-truth))) for pred, truth in zip(preds, y_test)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating Metric and Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the RMSE\n",
    "#print(np.sqrt(mean_squared_error(preds, y_test)))\n",
    "\n",
    "#plt.scatter(y_test, preds)\n",
    "\n",
    "#print(lr.coef_)\n",
    "#X_train.columns\n",
    "#X_train.LotFrontage.hist()\n",
    "#X_train.total_sf.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "scaler = standard_scaler.fit(X_train)\n",
    "#print(scaler.mean_)\n",
    "#print(X_train.mean())\n",
    "\n",
    "#print(scaler.var_)\n",
    "#print(X_train.var(ddof=0))\n",
    "#scaled_features = scaler.transform(X_train)\n",
    "#X_train_nrm = pd.DataFrame(scaled_features, index=X_train.index, columns=X_train.columns)\n",
    "#print(X_train_nrm.mean())\n",
    "#print(X_train_nrm.var(ddof=0))\n",
    "\n",
    "#lr_nrm = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "#lr_nrm.fit(X_train_nrm, y_train)\n",
    "\n",
    "# Test the model\n",
    "#X_test_nrm = pd.DataFrame(\n",
    "#    scaler.transform(X_test),\n",
    "#    index=X_test.index, columns=X_test.columns\n",
    "#)\n",
    "#print(X_test_nrm.mean())\n",
    "#preds_nrm = lr_nrm.predict(X_test_nrm)\n",
    "#print(lr.coef_)\n",
    "#print(lr_nrm.coef_)\n",
    "\n",
    "#np.isclose(preds, preds_nrm)\n",
    "#print(preds)\n",
    "#print(preds_nrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Manually, in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n, m = X_train.shape\n",
    "\n",
    "# Add an \"intercept\" term for beta_0\n",
    "#intercept_x = np.hstack((np.ones((n,1)), X_train))\n",
    "#print(intercept_x)\n",
    "\n",
    "#solution, sum_of_square_residuals, rank, singvals = np.linalg.lstsq(intercept_x, y_train)\n",
    "#print(solution)\n",
    "#print(lr.intercept_, lr.coef_)\n",
    "\n",
    "#preds_man = solution[0] * 1 + solution[1] * X_test.iloc[:, 0] + solution[2] * X_test.iloc[:, 1]\n",
    "#np.isclose(preds, preds_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ y = \\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some Notes\n",
    "* Gradient Descent (stochastic)\n",
    "* \"Linear\" in linear model: linear combination of input, weights, i.e. $y = \\beta X$\n",
    "* Overcomplex models (m>>n) tend to overfit\n",
    "* Some forms of linear regression penalize for complexity\n",
    "* **Categorical Variables**: http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some notes on model complexity\n",
    "* Very simple models **underfit**, leads to high **bias**\n",
    "* Very complex models **overfit**, leads to high **variance**\n",
    "* The goal is to differentiate the **signal** from the **noise**\n",
    "* (Highly recommended book on predictions: http://amzn.to/2DuydQ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to try and overcome these issues\n",
    "* **Never train and evaluate on the same set!**\n",
    "* We measure the performance of our model on the test set.\n",
    "* Validation curves (http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Most important\n",
    "* Guiding questions: What are the data saying?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
