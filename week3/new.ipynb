{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "## Data retrieval, preprocessing, and normalization for ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Outline\n",
    "  \n",
    "* Where do data come from? Data retreival.\n",
    "* Ideal datasets and data types\n",
    "* Common wrangling needs and implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Retrieving data from a remote web service in JSON format that gets converted to a python structure:\n",
    "def get_genome_sequence_ensembl(chromosome, start, end):\n",
    "    \"\"\" API described here http://rest.ensembl.org/documentation/info/sequence_region\"\"\"\n",
    "    url = 'https://rest.ensembl.org/sequence/region/human/{0}:{1}..{2}:1?content-type=application/json'.format(\n",
    "        chromosome, start, end)\n",
    "    r = requests.get(url, headers={\"Content-Type\": \"application/json\"}, timeout=10.000)\n",
    "    if r.ok:\n",
    "        return r.json()['seq']\n",
    "print(get_genome_sequence_ensembl(7, 200000,200100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pandas covers most of the data retrieval needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's read with pandas\n",
    "# Note that we do not even need to unzip the file before opening!\n",
    "brca_data = pd.read_table(FILENAME, sep=\"\\t\")\n",
    "brca_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas can even retrieve from an SQL database directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to \n",
    "# !pip install sqlalchemy\n",
    "# !pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "# Connect to UCSC genomic database\n",
    "engine = sa.create_engine('mysql+pymysql://genome@genome-mysql.cse.ucsc.edu/hg38', poolclass=sa.pool.NullPool)\n",
    "# select 3 SNPs from Chromosome Y\n",
    "pd.read_sql(\"SELECT * FROM snp147Common WHERE chrom='chrY' LIMIT 3\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pandas dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataframes are convenient containers for mixed data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas is *incredibly useful* for data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sklearn is happy to accept Pandas dataframes as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where did you get your data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas is built for exploratory analysis, visualization and stat tests / ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boilerplate\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "brca_data['normalized_expression_level'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a dataset: when are ready for ML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ideally, data are organized as a table: examples-vs-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data from multiple sources are combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Missing data are handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Features have been combined and manipulated as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Any data that need to be normalized have been normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data are of the correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at Boston housing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generated in-house (stored as CSVs, TSVs, SQL, proprietary, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_table(\"https://biof509.github.io/spring2019/_downloads/boston_data.csv\", sep=\",\")\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as a table: examples-vs-features~~\n",
    "* Data from multiple sources are combined\n",
    "* Missing data are handled\n",
    "* Features have been combined and manipulated as needed\n",
    "* Any data that need to be normalized have been normalized\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_second_floor = pd.read_table(\"https://biof509.github.io/spring2019/_downloads/boston_second_floor.csv\", sep=\",\")\n",
    "boston_second_floor.head()\n",
    "#boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine boston and boston second floor\n",
    "boston = pd.merge(boston, boston_second_floor, on=\"Id\")\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some additional data\n",
    "boston3 = pd.read_table(\"https://biof509.github.io/spring2019/_downloads/boston_additional.csv\", sep=\",\")\n",
    "boston3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.tail(10)\n",
    "#boston.shape\n",
    "#boston.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as a table: examples-vs-features~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* Missing data are handled\n",
    "* Features have been combined and manipulated as needed\n",
    "* Any data that need to be normalized have been normalized\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Collaborators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "There are a number of ways to handle missing data:\n",
    "\n",
    "* Drop all records with a value missing (simplest, but can lead to bias)\n",
    "* Substitute all missing values with an approximated value (usually depends on data and algorithm)\n",
    "* Add additional feature indicating when a value is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all records with missing data\n",
    "#boston.isnull().tail()\n",
    "# boston.isnull().sum()\n",
    "# boston.isnull().sum().sum()\n",
    "#boston.tail()\n",
    "#boston.dropna().tail()\n",
    "boston.dropna().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute missing values\n",
    "# boston.fillna(\"Value!\").tail()\n",
    "boston.fillna({\"2ndFlrSF\": \"Value1!\", \"LotFrontage\": \"Value2!\"}).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute missing values with mean\n",
    "# print(boston.mean())\n",
    "#boston.fillna(boston.mean()).tail()\n",
    "#boston.fillna(boston.median()).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column indicating missing values\n",
    "# boston[\"2ndFlrSF\"].isnull()\n",
    "#boston[\"missing_second_floor\"] = boston[\"2ndFlrSF\"].isnull()\n",
    "# boston.tail()\n",
    "# boston = boston.fillna(boston.mean())\n",
    "# boston.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to upgrade Scikit-learn (and restart Jupyter kernel afterwards) to use Imputer\n",
    "# !pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation is a general technique for \"guessing\" appropriate missing values\n",
    "# It could be implemented as a complex ML regression algorithm or a simple 'take an average' strategy.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit_transform(boston[[\"LotFrontage\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit_transform(boston[[\"LotFrontage\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to decide how to treat missing data?\n",
    "* Very data-dependent!\n",
    "* Decisions need to be justified and documented\n",
    "* Implement missing data preprocessing in a reproducible way (python script)\n",
    "* Don't create data from nothing\n",
    "* Iris example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Public sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as a table: examples-vs-features~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* ~~Missing data are handled~~\n",
    "* Features have been combined and manipulated as needed\n",
    "* Any data that need to be normalized have been normalized\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boston.head()\n",
    "boston[\"total_sf\"] = boston[\"1stFlrSF\"] + boston[\"2ndFlrSF\"]\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.head()\n",
    "boston = boston.replace({\"Abnorml\": \"abnormal\", \"Normal\": \"normal\"})\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as a table: examples-vs-features~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* ~~Missing data are handled~~\n",
    "* ~~Features have been combined and manipulated as needed~~\n",
    "* Any data that need to be normalized have been normalized\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "* What is it?\n",
    "* Why do it? (data sources, feature distributions)\n",
    "* Types?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Many machine learning algorithms expect features to have similar distributions and scales.\n",
    "\n",
    "A classic example is gradient descent, if features are on different scales some weights will update faster than others because the feature values scale the weight updates.\n",
    "\n",
    "There are two common approaches to normalization:\n",
    "\n",
    "* Z-score standardization\n",
    "* Min-max scaling\n",
    "\n",
    "#### Z-score standardization\n",
    "\n",
    "Z-score standardization rescales values so that they have a mean of zero and a standard deviation of 1. Specifically we perform the following transformation:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "#### Min-max scaling\n",
    "\n",
    "An alternative is min-max scaling that transforms data into the range of 0 to 1. Specifically:\n",
    "\n",
    "$$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Min-max scaling is less commonly used but can be useful for image data and in some neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = (boston['1stFlrSF'] - boston['1stFlrSF'].mean()) / boston['1stFlrSF'].std()\n",
    "# boston['1stFlrSF'].hist()\n",
    "# boston.head()\n",
    "## boston.total_sf.hist()\n",
    "from sklearn.preprocessing import scale, StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(boston[['1stFlrSF']]))\n",
    "#scaled_size = pd.Series(scale(boston.total_sf))\n",
    "#scaled_size.hist()\n",
    "#scaled_size.mean()\n",
    "#scaled_size.std(ddof=0)\n",
    "#boston[\"normalized_total_sf\"] = scaled_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit_transform(boston[['1stFlrSF']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other preprocessing / normalization techniques and thoughts\n",
    "* http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "* http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as a table: examples-vs-features~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* ~~Missing data are handled~~\n",
    "* ~~Features have been combined and manipulated as needed~~\n",
    "* ~~Any data that need to be normalized have been normalized~~\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripting data retrieval improves reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boston.head()\n",
    "import numpy as np\n",
    "\n",
    "# boston[\"1stFlrSF\"].mean(skipna=False)\n",
    "boston[\"CentralAir_bool\"] = boston[\"CentralAir\"] == \"Y\"\n",
    "# boston.head()\n",
    "# boston[\"SaleCondition\"].dtype\n",
    "#boston[\"SaleCondition\"].head()\n",
    "boston[\"SaleCondition\"].astype(\"category\").dtype\n",
    "#boston[\"SaleCondition\"] = boston[\"SaleCondition\"].astype(\"category\")\n",
    "#boston[\"SaleCondition\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit_transform(['yes', 'yes', 'no', 'no'])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.fit_transform(['yes', 'yes', 'no', 'no', 'maybe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "sparse_matrix = ohe.fit_transform(boston[['SaleCondition', 'CentralAir_bool']])\n",
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example of categorical data conversion to boolean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame([[0,1,2,3,4,5,6],\n",
    "                  [2,np.nan,7,4,9,1,3],\n",
    "                  [0.1,0.12,0.11,0.15,0.16,0.11,0.14],\n",
    "                  [100,120,np.nan,127,130,121,124],\n",
    "                  ['Green','Red','Blue','Blue','Green','Red','Green']], ).T\n",
    "x.columns = ['A', 'B', 'C', 'D', 'E']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = x.copy()\n",
    "for val in x['E'].unique():\n",
    "    x_cat['E_{0}'.format(val)] = x_cat['E'] == val\n",
    "x_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option to have one feature per color is to use Pivot\n",
    "# Note that it will create missing data:\n",
    "x.pivot(index='A', columns='E', values='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as a table: examples-vs-features~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* ~~Missing data are handled~~\n",
    "* ~~Features have been combined and manipulated as needed~~\n",
    "* ~~Any data that need to be normalized have been normalized~~\n",
    "* ~~Data are of correct type (e.g. categorical vs continuous, boolean vs int)~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other types of data storage\n",
    "* Image\n",
    "* Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to:\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image\n",
    "\n",
    "Datasets with images also need to follow samples-by-features format.\n",
    "Features in this case are pixels and their intensities. For black and white images intensities are binary. For grayscale they could be integer or floating point numbers. Color images are usually represented as multiple images - one for each color channel (e.g. red / green / blue).\n",
    "\n",
    "Thus each image is represented as a one dimensional array, which is exactly what's needed for ML applications. To visualize it, however, we need to change its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "dataset = fetch_olivetti_faces()     \n",
    "print(\"Dimensionality samples x features\", dataset.data.shape)\n",
    "\n",
    "# first image - pixel intensities\n",
    "dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping to visualize\n",
    "plt.imshow(dataset.data[0].reshape(64, 64), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of normalization of an image\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "image = dataset.data[0].reshape(64, 64)\n",
    "normalized_image = Binarizer(threshold=0.6).fit_transform(image)\n",
    "plt.imshow(normalized_image, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text\n",
    "\n",
    "Text has also to be transformed to samples-by-features format.\n",
    "In the simplest case each document is a sample and ocurrence of words are its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "emails = fetch_20newsgroups(subset='train', categories=['sci.med'], shuffle=True, random_state=0)\n",
    "print(\"Number of documents\", len(emails.data))\n",
    "print(\"Beginning of the first document\", emails.data[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every document we count word ocurrence:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "emails_in_ML_format = CountVectorizer().fit_transform(emails.data)\n",
    "print(emails_in_ML_format.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this is how the first document looks like:\n",
    "emails_in_ML_format[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading a data file from a remote repository\n",
    "import urllib\n",
    "\n",
    "URL = \"https://dcc.icgc.org/api/v1/download?fn=/release_18/Projects/BRCA-US/protein_expression.BRCA-US.tsv.gz\"\n",
    "FILENAME = \"brca_protein_expression.tsv.gz\"\n",
    "\n",
    "urllib.request.urlretrieve(URL, FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scraping tools such as Mechanize and BeautifulSoup allow extraction of data from websites\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}